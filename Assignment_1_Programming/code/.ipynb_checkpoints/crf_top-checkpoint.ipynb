{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/nikolaos/Workspaces/AML/Assignment_1_Programming/code\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "import lib_liblinear.python.liblinearutil as lbl\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.optimize as opt\n",
    "from PIL import Image \n",
    "dir_path = Path.cwd()\n",
    "\n",
    "print(dir_path)\n",
    "sys.path.insert(0, dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done reading! Sizes are X: (12800,), W:(3328,),T: (676,). Total: 16804\n",
      "Done reshaping! Sizes now are X: (100, 128), W:(128, 26),T: (26, 26)\n"
     ]
    }
   ],
   "source": [
    "dataPath = os.path.join(dir_path, '../data')\n",
    "resPath = os.path.join(dir_path, '../results')\n",
    "letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "# Load decode Input\n",
    "decInputFile = os.path.join(dataPath, 'decode_input.txt')\n",
    "with open(decInputFile, 'r') as f:\n",
    "    inData = f.read().splitlines()\n",
    "    inX = np.asarray(inData[0:12800], dtype = np.float)     # input is 100 letters @ 128 = 12800, or 0-12799. NOTE: Recall Python slice is [).\n",
    "    inW = np.asarray(inData[12800:16128], dtype = np.float) # wieths are 26 * 128 = 3328, or 12800-16127\n",
    "    inT = np.asarray(inData[16128:], dtype = np.float)      # The remaing are the transition probs.\n",
    "print(\"Done reading! Sizes are X: {}, W:{},T: {}. Total: {}\".format(inX.shape, inW.shape, inT.shape, inX.size+inW.size+inT.size))\n",
    "\n",
    "# Reshape arrays\n",
    "# Reshape T to 26x26\n",
    "inT = np.reshape(inT,(26,26)) \n",
    "inX = np.reshape(inX, (-1,128))\n",
    "inW = np.reshape(inW, (128,-1))\n",
    "featSize = 128\n",
    "numOfLabels = 26\n",
    "print(\"Done reshaping! Sizes now are X: {}, W:{},T: {}\".format(inX.shape, inW.shape, inT.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1.c Decoder Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'crf_decode2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-c725f41c98c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecSeq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxObj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mdecSeq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmaxObj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcrf_decode2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaxObj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'crf_decode2' is not defined"
     ]
    }
   ],
   "source": [
    "# Max-Prod algorithm!\n",
    "def crf_decode_max_prod(seq, w, t):\n",
    "    \"\"\" DESCRIPTION: Max-Prod algorithm. Not used for this assignment. Use max-sum instead!\n",
    "    \"\"\"\n",
    "    # Init\n",
    "    seqLen  = seq.shape[0]\n",
    "    dataLen = w.shape[1]\n",
    "    cMat    = np.zeros((seqLen, dataLen))\n",
    "    bMat    = np.matmul(seq, w) # this gives the probability of observing each i standalone\n",
    "    maxIdx  = np.zeros((seqLen, dataLen), dtype = np.int)\n",
    "    decSeq  = []\n",
    "    maxObj  = 0\n",
    "    ci = np.zeros((26,26))\n",
    "    \n",
    "    # Get step 0 estimates\n",
    "    step0    = np.matmul(seq[0,:], w)\n",
    "    c1 = np.multiply(step0,t)      # the previous node potential x edje potenital for all i->j pairs\n",
    "    maxIdx[0,:] = np.argmax(c1, axis = 0) #1x26 the max elem indexes\n",
    "    c1 = c1[maxIdx[0,:],np.arange(c1.shape[1])] #1x26 select the max element, as indexed by maxIdx, for every column\n",
    "    cMat[0,:] = c1\n",
    "    print(\"Input shapes: sequence: {}, weights: {}, T matrix:{}\".format(seq.shape,w.shape,t.shape))\n",
    "    \n",
    "    # Perform a pass over steps 1-99 or 2-100 in 1-index notation!\n",
    "    for i in range(1,seqLen):\n",
    "        fi  = bMat[i,:]              # get likelihood of i observation being any letter as standalone \n",
    "        cYi = cMat[i-1,:]            # i-1 step's potential msg to this node i\n",
    "        cYj = np.multiply(fi, t)     # mul the likelihood of each letter with all transition probs.\n",
    "        maxIdx[i,:] = np.argmax(cYj, axis = 0) #1x26 the max elem indexes\n",
    "        cYj = cYj[maxIdx[i,:],np.arange(cYj.shape[1])] #1x26 select the max element, as indexed by maxIdx, for every column\n",
    "        cYj = np.multiply(cYj, cYi)  # multiply current cYj = g_yj->yi* fj with pervious node's(i-1) msgs to yield the msg node i will wend to i+1\n",
    "        cMat[i,:] = cYj              # store this node's msg, in order to send it to i+1 on next step!\n",
    "        #if i == 1:\n",
    "        #    print(cYj, cYj.shape, fi.shape)\n",
    "    maxObj = np.max(cYj)             # max objective value is the maximum of the lasp step's messages or potentials Y_end \n",
    "    # Backward pass.\n",
    "    # At each step, the element e at slot i is the label that most likely lead to the lable represented as i.\n",
    "    # So at i.e if 95,5 = 11, then that means that the most probable letter that leads to f(idx 5) if letter 11 or l (label 11). All indexing is 0 based.\n",
    "    decSeq     = np.zeros((seqLen,1), dtype = np.int)\n",
    "    lastMax    = np.argmax(maxIdx[-1,:]) # get the most probable last element. use this to recurse and find the most prob sequence\n",
    "    decSeq[-1] = lastMax\n",
    "    for i in range(seqLen-2,-1,-1):\n",
    "        #print(\"i is \", i, \"decSeq of \", i+1, \"is\", decSeq[i+1])\n",
    "        curMax = maxIdx[i,decSeq[i+1]]\n",
    "        decSeq[i] = curMax\n",
    "        \n",
    "    return decSeq, maxObj\n",
    "    \n",
    "decSeq,maxObj = crf_decode2(inX, inW, inT)\n",
    "print(maxObj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "206.68391391587812\n"
     ]
    }
   ],
   "source": [
    "# Max-Sum algorithm!\n",
    "def crf_decode(seq, w, t):\n",
    "    \"\"\" DESCRIPTION: Using max-sum algorithm to decode. THis function will use the forward pass to compute\n",
    "                     the most probable last letter, at place m for a seq of length m. Then using that knowledge\n",
    "                     it will go backwards at m-1 extracting the character that was most likely to lead to the most probable \n",
    "                     character at place m. It will recurcively continue until we have a candidate at place m=0 and return\n",
    "                     the labels that we are most confident they match the input.\n",
    "        ARGUMENTS: seq(ndarray): a sequqnce that contains aoiur input data.\n",
    "    \"\"\"\n",
    "    # Init\n",
    "    seqLen  = seq.shape[0]\n",
    "    dataLen = w.shape[1]\n",
    "    cMat    = np.zeros((seqLen, dataLen))\n",
    "    bMat    = np.matmul(seq, w) # this gives the probability of observing each i standalone\n",
    "    maxIdx  = np.zeros((seqLen, dataLen), dtype = np.int)\n",
    "    decSeq  = []\n",
    "    maxObj  = 0\n",
    "    ci = np.zeros((26,26))\n",
    "    \n",
    "    # Get step 0 estimates\n",
    "    step0    = np.matmul(seq[0,:], w)\n",
    "    c1 = np.multiply(step0,t)      # the previous node potential x edje potenital for all i->j pairs\n",
    "    maxIdx[0,:] = np.argmax(c1, axis = 0) #1x26 the max elem indexes\n",
    "    c1 = c1[maxIdx[0,:],np.arange(c1.shape[1])] #1x26 select the max element, as indexed by maxIdx, for every column\n",
    "    cMat[0,:] = 0\n",
    "    #print(\"Input shapes: sequence: {}, weights: {}, T matrix:{}\".format(seq.shape,w.shape,t.shape))\n",
    "    \n",
    "    # Perform a pass over steps 1-99 or 2-100 in 1-index notation!\n",
    "    for i in range(1,seqLen):\n",
    "        fi  = bMat[i-1,:].reshape(-1,1)              # get likelihood of i observation being any letter as standalone \n",
    "        cYi = cMat[i-1,:].reshape(-1,1)            # i-1 step's potential msg to this node i\n",
    "        cYj = fi + t + cYi     # mul the likelihood of each letter with all transition probs.\n",
    "        #if i ==1:\n",
    "            #print(cYi.shape,cYj.shape)\n",
    "        maxIdx[i,:] = np.argmax(cYj, axis = 0) #1x26 the max elem indexes\n",
    "        cYj = cYj[maxIdx[i,:],np.arange(cYj.shape[1])] #1x26 select the max element, as indexed by maxIdx, for every column\n",
    "        #cYj = np.multiply(cYj, cYi)  # multiply current cYj = g_yj->yi* fj with pervious node's(i-1) msgs to yield the msg node i will wend to i+1\n",
    "        cMat[i,:] = cYj              # store this node's msg, in order to send it to i+1 on next step!\n",
    "        #if i == 1:\n",
    "        #    print(cYj, cYj.shape, fi.shape)\n",
    "    maxObj = np.max(cYj)             # max objective value is the maximum of the lasp step's messages or potentials Y_end \n",
    "    # Backward pass.\n",
    "    # At each step, the element e at slot i is the label that most likely lead to the lable represented as i.\n",
    "    # So at i.e if 95,5 = 11, then that means that the most probable letter that leads to f(idx 5) if letter 11 or l (label 11). All indexing is 0 based.\n",
    "    decSeq     = np.zeros((seqLen,1), dtype = np.int)\n",
    "    lastMax    = np.argmax(maxIdx[-1,:]) # get the most probable last element. use this to recurse and find the most prob sequence\n",
    "    decSeq[-1] = lastMax\n",
    "    for i in range(seqLen-2,-1,-1):\n",
    "        #print(\"i is \", i, \"decSeq of \", i+1, \"is\", decSeq[i+1])\n",
    "        curMax = maxIdx[i,decSeq[i+1]]\n",
    "        decSeq[i] = curMax\n",
    "    # Remember that python indexes from 0. Labels are 1-26 so we must add 1 to compensate\n",
    "    return decSeq+1, maxObj\n",
    "    \n",
    "decSeq,maxObj = crf_decode(inX, inW, inT)\n",
    "print(maxObj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def write_dec_seq_to_file(seq, resPath, label='decode_output.txt'):\n",
    "    \"\"\" DESCRIPTION: Write the decoded results to file\n",
    "    \"\"\"\n",
    "    # Create results dir if ti does not exists (Python3.2+)\n",
    "    #resPath = os.path.join(dir_path, 'results')\n",
    "    os.makedirs(resPath, exist_ok=True) \n",
    "    # Form file name and write output\n",
    "    resFile = os.path.join(resPath, label)\n",
    "    np.savetxt(resFile, seq.astype(int),fmt='%d')\n",
    "# ------------------------------------------------------------------    \n",
    "write_dec_seq_to_file(deqSeq, os.path.join(dir_path, 'results'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2\n",
    "## Marginals Computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load Data for all formats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "# Part 3 Benchamrking with SVM!\n",
    "modelLabels = ['CRF', 'SVM-HMM', 'SVM-MC']\n",
    "# Define where svm_hmm lib and executables are\n",
    "libPath = os.path.join(dir_path,'svm_hmm_linux64')\n",
    "learnerPath = os.path.join(libPath, 'svm_hmm_learn')\n",
    "testerPath  = os.path.join(libPath, 'svm_hmm_classify')\n",
    "# data for the hmm svm\n",
    "svmTrainData = os.path.join(dataPath,'train_struct.txt')\n",
    "svmTestData = os.path.join(dataPath,'test_struct.txt')\n",
    "# liblinear svm data\n",
    "libSVMTrainData = os.path.join(dataPath, 'liblinear_svm_train.txt')\n",
    "libSVMTestData  = os.path.join(dataPath, 'liblinear_svm_test.txt')\n",
    "# make a file of true classes if they dont exists, without hte other stuff\n",
    "sanTrainTruthFile = os.path.join(dataPath, 'sanitizedTrainTruth.txt')\n",
    "sanTestTruthFile = os.path.join(dataPath, 'sanitizedTestTruth.txt')\n",
    "# Load the data in CRF readable format\n",
    "crfTrainFile = os.path.join(dataPath, 'train.txt')\n",
    "crfTestFile  = os.path.join(dataPath, 'test.txt')\n",
    "\n",
    "def load_crf_data(trainFile, testFile):\n",
    "    \"\"\" DESCRIPTION: This function will read the files and return the data as numpy arrays, without the additional supporting information\n",
    "                     which in the provided files arethe first 5 characters describing word indx, letter indx end letter indicator etc.\n",
    "                     Refer to fields_crf.txt for more info on those\n",
    "        RETURNS: ndarrays: The numpy arrays containing just the data. Shape is  numOfLetters x 128\n",
    "    \"\"\"\n",
    "    with open(trainFile,'r') as tr, open(testFile,'r') as ts:\n",
    "        trainD = np.asarray([[int(y) for y in x.split(' ')[5:]] for x in tr.read().splitlines()])\n",
    "        testD  = np.asarray([[int(y) for y in x.split(' ')[5:]] for x in ts.read().splitlines()])\n",
    "        \n",
    "        #print(len(trainD[0]),len(testD[0]))\n",
    "    return trainD, testD\n",
    "#-------------------------------------------------------------------\n",
    "trainD, testD = load_crf_data(crfTrainFile, crfTestFile)\n",
    "\n",
    "def get_sanitized_labels(inFile, sanTruthFile = None):\n",
    "    # Load the true classes and word-letter indexex (2nd column of file)\n",
    "    # Get just the true labels and word indexes for all letters without features and the rest\n",
    "    if  not os.path.exists(sanTruthFile) or sanTruthFile is None:\n",
    "        with open(inFile,'r') as o, open(sanTruthFile,'w') as s:\n",
    "            d =[x.split(' ') for x in o.readlines()]   # split file into a list of lists of tokens\n",
    "            sanLabel =[x[0] for x in d]                # get all labels. Its the first token of each line\n",
    "            sanWIdx = [x[1].split(':')[1] for x in d]  # get all word indexes. It is the number after the : of the second token.\n",
    "            # using list comprehension + zip() \n",
    "            # interlist element concatenation \n",
    "            sanData = [' '.join((i,j)) for i, j in zip(sanLabel, sanWIdx)]  \n",
    "            print(sanData[0:30])\n",
    "            s.write(\"\\n\".join(sanData))\n",
    "            sanData = np.asarray(sanData.split(), dtype = np.int)\n",
    "            print(sanData[0:30])\n",
    "    else:\n",
    "        sanData = np.loadtxt(sanTruthFile, dtype = np.int, delimiter = ' ')\n",
    "    return sanData\n",
    "#-------------------------------------------------------------------\n",
    "sanTrainLabels = get_sanitized_labels(svmTrainData, sanTrainTruthFile)        \n",
    "sanTestLabels = get_sanitized_labels(svmTestData, sanTestTruthFile)        \n",
    "#gTruth = np.asarray([x.split(' ')[0] for x in open(sanTruthFile).readlines()]) # keep in mind that readlines retunrs the \\n character\n",
    "\n",
    "#-------------------------------------------------------------------\n",
    "# transform data strcuture to LIBLINEAR format. Essentially we need to only remove the qid:x part\n",
    "if not  os.path.exists(libSVMTestData):\n",
    "    with open(svmTestData,'r') as i, open(libSVMTestData,'w') as o:\n",
    "        d =[x.split(' ') for x in i.readlines()]   # split file into a list of lists of tokens\n",
    "        for row in d:\n",
    "            del row[1]\n",
    "        #print(d[0:20])\n",
    "        d = [' '.join(row[0:-1]) for row in d]\n",
    "        o.write(\"\\n\".join(d))\n",
    "        \n",
    "if not os.path.exists(libSVMTrainData):\n",
    "    with open(svmTrainData,'r') as i, open(libSVMTrainData,'w') as o:\n",
    "        d =[x.split(' ') for x in i.readlines()]   # split file into a list of lists of tokens\n",
    "        for row in d:\n",
    "            del row[1]\n",
    "        d = [' '.join(row[0:-1]) for row in d]\n",
    "        #print(d[0:20])\n",
    "        o.write(\"\\n\".join(d))    \n",
    "d = 0\n",
    "# --------------------------------------------------------------\n",
    "\n",
    "def extract_word_idxs(inData):\n",
    "    \"\"\" DESCRIPTION: Expects a file that  has a per-line word idx. That is each row is the word idx the letter belongs to.\n",
    "        ARGUMENTS: inData (ndarray): A numpy array containing the letter-word idxs. Each line is the letter idx in data and the value is\n",
    "                                     the word it belongs to. So line 0, with value = 1 means letter 0 belongs to word 1.\n",
    "        RETURNS: wordIdxs(ndarray): A numpy arra 1D, that holds at each location the end limit of that word. So at location i=2, the bvalue v=17\n",
    "                                    means that word 2 ends at location 17. So when indexing with python's [) ranges, we just use word2 = data[endWord1, 17]\n",
    "    \"\"\"\n",
    "    wordIdxs = np.zeros(inData[-1], dtype=np.int)\n",
    "    curIdx = 1\n",
    "    for i, runIdx in enumerate(inData):\n",
    "        if curIdx != runIdx:\n",
    "            wordIdxs[curIdx] = i\n",
    "            curIdx = runIdx\n",
    "    #print(wordIdxs.shape, wordIdxs[-20:]) \n",
    "    return wordIdxs\n",
    "# ------------------------------------------------------------------\n",
    "#\n",
    "def append_label_to_crf_data(trainD, testD, trainL, testL):\n",
    "    \"\"\" DESCRIPTION: This function is needed to append letter and wordwise labels to raw data. optimize.py file provided accesses data in a different \n",
    "                     manner, passing only one table for data input, assuming the labels are carried along. As such, we need to concat the 2 matrices.\n",
    "    \n",
    "    \"\"\"\n",
    "    labTrainD = np.append(trainL, trainD, axis = 1) \n",
    "    labTestD = np.append(testL, testD, axis = 1) \n",
    "    return labTrainD, labTestD\n",
    "#Extract the word Indexes.\n",
    "wordIdxs = extract_word_idxs(sanTrainLabels[:,1])\n",
    "wordTestIdxs = extract_word_idxs(sanTestLabels[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 26)"
      ]
     },
     "execution_count": 261,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inW.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute log Marginals and log p(Y|X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 26)"
      ]
     },
     "execution_count": 263,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def compute_log_dist(seq, w, t, labels= None):\n",
    "    \"\"\" DESCRIPTION: THis function will compute the log marginals Pyi and Pyy.\n",
    "        ARGUMENTS: seq(ndarray; DIMS: seqLen x featLen): Input sequence to compute the distribution on. All \n",
    "                   members have to have same length.  \n",
    "                   w(ndarray DIMS: featLenx numOfLabels) Input node weights matrix.\n",
    "                   t(ndarray DIMS: numOflabelsxnumOflabels): Input edge weights (Transition matrix.)\n",
    "        RTETURNS: dist (ndarra; DIMS: seqLenx labelLen) The marginal distribution of each label as each seq spot s.\n",
    "                  distT (ndarray): The marginal of the edge probabilities. [DIMS: seqlen x numOfLabels x numOfLabels]\n",
    "                  Z (scalar): Partition function's result, computed on this sequence length.\n",
    "    \"\"\"\n",
    "    dist = []\n",
    "    # Init\n",
    "    seqLen  = seq.shape[0]\n",
    "    dataLen = w.shape[1] # feature length\n",
    "    fMat    = np.zeros((seqLen, dataLen))\n",
    "    oMat    = np.matmul(seq, w) # this gives the probability of observing each i standalone\n",
    "    maxIdx  = np.zeros((seqLen, dataLen), dtype = np.int)\n",
    "    decSeq  = []\n",
    "    maxObj  = 0\n",
    "    ci = np.zeros((26,26))\n",
    "    # ---------------------------\n",
    "    # Compute forward messages (f potentials) \n",
    "    # Get step 0 estimates\n",
    "    step0    = np.matmul(seq[0,:], w)\n",
    "    c1 = np.multiply(step0,t)      # the previous node potential x edje potenital for all i->j pairs\n",
    "    c1 = c1[maxIdx[0,:],np.arange(c1.shape[1])] #1x26 select the max element, as indexed by maxIdx, for every column\n",
    "    #cMat[0,:] = c1\n",
    "    fMat[0,:] = 0\n",
    "    #print(\"Input shapes: sequence: {}, weights: {}, T matrix:{}\".format(seq.shape,w.shape,t.shape))\n",
    "    \n",
    "    # Perform a pass over steps 1-99 or 2-100 in 1-index notation!\n",
    "    # Recall that bmat is m x 26, each row a letter-wsie confidence vector. Similarly for cMat which is message of node i to i+1.\n",
    "    for i in range(1,seqLen):\n",
    "        fPrev  = oMat[i-1,:].reshape(-1,1)   # get likelihood of i observation being any letter as standalone,as a column vector\n",
    "        fYPrev = fMat[i-1,:].reshape(-1,1)   # i-1 step's potential msg to this node i. f_y-1 in the assignment.\n",
    "        cYi = np.exp(fPrev.transpose() + t + fYPrev.transpose())             # Turn weighted features into potentials.\n",
    "        cYi = np.sum(cYi, axis =0) # sum the likelihood of each letter with all transition probs.\n",
    "        #if i == 1:\n",
    "            #print(cYi.shape)\n",
    "            #pint(fPrev.shape,cYi, fPrev, t)\n",
    "        #cYi = np.exp(cYi + fYPrev)             # Turn weighted features into potentials.\n",
    "        cYi = np.log(cYi)\n",
    "        #print(fPrev.shape,cYi.shape, cYPrev.shape)\n",
    "        fMat[i,:] = cYi              # store this node's msg, in order to send it to i+1 on next step!\n",
    "        \n",
    "    \n",
    "    # Backward pass.\n",
    "    # ---------------------------\n",
    "    bMat    = np.zeros((seqLen, dataLen)) # hold all the backward messages for the sequence, for each position s.\n",
    "    # Compute b_i's\n",
    "    bMat[seqLen-1,:] = 0\n",
    "    for i in range(seqLen-2, -1,-1):\n",
    "        fNext  = oMat[i+1,:].reshape(-1,1) # get likelihood of i+1 observation being any letter as standalone \n",
    "        bYNext = bMat[i+1,:].reshape(-1,1) # i+1 step's potential msg to this node i. f_y-1 in the assignment.\n",
    "        cYi = np.exp(fNext + t + bYNext) # sum the likelihood of each next letter letter i+1, along with the trans probs for each letter i in this step \n",
    "        cYi = np.sum(cYi, axis=1)        # Turn weighted features into potentials.\n",
    "        cYi = np.log(cYi)\n",
    "        bMat[i,:] = cYi              # store this node's msg, in order to send it to i+1 on next step!\n",
    "        \n",
    "    # Compute P(yi) for all sequence spots s and all letters l (if |Y| = l, that is the cardinality of labels is l.)    \n",
    "    # -------------------------\n",
    "    dist = fMat + bMat + oMat\n",
    "    #print(np.sum(np.exp(oMat[-1,:] + fMat[-1,:])))\n",
    "    #print(np.exp(oMat[-1,:] + fMat[-1,:]))\n",
    "    Zf = np.log(np.sum(np.exp(oMat[-1,:] + fMat[-1,:])))\n",
    "    Zb = np.log(np.sum(np.exp(oMat[0,:] + bMat[0,:])))\n",
    "    # Sanity check. This 2 should be the same. THese are Z.\n",
    "    #print(Zf,Zb)\n",
    "    logZ = Zf\n",
    "    \n",
    "    # Locally normalize distribution computation\n",
    "    dist /= np.sum(dist, axis = 1, keepdims =1)    \n",
    "    \n",
    "    # Pyy transition comuptation\n",
    "    # --------------------------\n",
    "    distT = np.zeros((seqLen-1,t.shape[0], t.shape[1]),dtype=np.float)\n",
    "    pYY = np.zeros((t.shape))\n",
    "    for s in range(0, seqLen-1):\n",
    "        #print(np.tile(fMat[s,:], (t.shape[0],1)).shape)\n",
    "        termB = np.tile(oMat[s,:], (t.shape[0],1)) + np.tile(oMat[s+1,:].reshape(-1,1), (1,t.shape[0])) + t\n",
    "        pYY = np.tile(fMat[s,:], (t.shape[0],1)) + np.tile(bMat[s+1,:].reshape(-1,1), (1,t.shape[0])) + termB\n",
    "        distT[s,:,:] = pYY / np.sum(pYY, axis = 1, keepdims =1)\n",
    "        #if s ==0:\n",
    "            #print(np.sum(pYY, axis = 1, keepdims =1))\n",
    "            #print(pYY/ np.sum(pYY, axis = 1, keepdims =1))\n",
    "            #print(distT[s].sum(axis=1)) # this should be 1's as each row should be a valid distribution\n",
    "            \n",
    "    # Compute lg p(y|X)\n",
    "    # --------------------------\n",
    "    if labels is not None:\n",
    "        logPy_X = 0\n",
    "        # Select the column holding the confidence of the true label for each datum (row)\n",
    "        # Remember, labels are indexed 1-26, python indexes 0-25; hence -1 is required\n",
    "        #print(oMat.shape, labels.shape)\n",
    "        wSum, tSum = np.sum(oMat[np.arange(len(labels)),labels-1]), np.sum(inT[labels[0:-1]-1, labels[1:]-1]) \n",
    "        res = (wSum+tSum) - logZ\n",
    "        #print(res)\n",
    "        #print(inT[labels[0:-1]-1, labels[1:]-1])\n",
    "        \n",
    "    return dist, distT, logZ, logPy_X\n",
    "\n",
    "# Try for just one word, sanity run.\n",
    "wordStart = 0\n",
    "wordEnd = wordIdxs[1]\n",
    "logPY, logPYY, Z, logPy_X = compute_log_dist(trainD[wordStart:wordEnd,:], inW, inT, labels = sanData[wordStart:wordEnd,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Gradients for Wy and Tyi,yi+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6, 128) (6,) (6, 26)\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------------\n",
    "def compute_word_grads(inSeq, labels, pY, pYY):\n",
    "    gradW = np.zeros((pY.shape[1], inSeq.shape[1])) # store grad of w as column:\n",
    "    gradY = np.zeros((pY.shape[1], pY.shape[1])) # square matrix of symbol-to-symbol transitions\n",
    "    #print(\"Input sizes for grad compute-> data:{}, pY:{}, pYY:{}\".format(seq.shape, pY.shape, pYY.shape))\n",
    "    #print(pY.sum(axis=1))\n",
    "    # Apply the eq gradWs = SUM_s {I[ys=y]- p(y|x)}xs \n",
    "    # So for each label, we find its contribution and then sum that label's controibution over all image\n",
    "    # So if label1 = a, then the contribution of x1 is contrib1_a, if labels2 = b we have contri2_a. To find the \n",
    "    # total grad for a we sum all the constribs fow a: contrib_s_a, where s runs from 0: sequence length!\n",
    "    for i in range(0, pY.shape[1]):\n",
    "        diff = ((labels == (i+1)).astype(int) - pY[:,i])  # Indicator func - p(y|X)\n",
    "        g = np.multiply(diff.reshape(inSeq.shape[0],1),inSeq)            # the above * xs (that is all data points in the sequence. We get a seqLenx128 matrix) \n",
    "        #if i == 11:  # Sanity print\n",
    "        #    print(g.shape)\n",
    "        #    print(labels[1])\n",
    "        #    print(inSeq[1,:])\n",
    "        #    print(g[1,:])\n",
    "        #    print(diff)\n",
    "        gradW[i] = np.sum(g, axis = 0) # accumulate grad on pixel positions over all sequence positions i. DIMS: 1x128 for each label\n",
    "     \n",
    "    # Grad T Computation\n",
    "    # ----------------------------\n",
    "    # Compute Indicator function I. It will be seqLen x 26x26. It has 1 in s,i,j if character at s,s+1 = i,j\n",
    "    # Dims = seqLen-1x26x26\n",
    "    indFunc = np.zeros((pYY.shape[0], pYY.shape[1], pYY.shape[2]))\n",
    "    for s in range(inSeq.shape[0]-1):\n",
    "        indFunc[s, labels[s]-1, labels[s+1]-1] = 1 # the -1 here is needed as labels are from 1-26 but T matrix is 26x26 so indexed as 0:25x:25\n",
    "    # Actual computation    \n",
    "    #for i in range(0, pY.shape[1]):\n",
    "    #    for j in range(0, pY.shape[1]):\n",
    "    #print(pYY[0])\n",
    "    #print(indFunc - pYY)\n",
    "    gradT = np.sum(indFunc - pYY,axis=0)\n",
    "    #print(gradT.shape)        \n",
    "    return gradW, gradT\n",
    "# ----------------------------------------------------------------\n",
    "# 1 word sanity run\n",
    "gradW, gradY = compute_word_grads(trainD[wordStart:wordEnd],sanData[wordStart:wordEnd,0], logPY, logPYY)\n",
    "#scipy.optimize.check_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute log P(y|X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computting average gradients for file /home/nikolaos/Workspaces/AML/Assignment_1_Programming/code/../data/train.txt. Data size: (25953, 128)\n"
     ]
    }
   ],
   "source": [
    "def compute_log_py(seq, w,t,logZ, labels):\n",
    "    \"\"\" DESCRIPTION: Compute log p(y|X) for a seq given the labels, node and edge weights. This is a standalone version\n",
    "                     of the code segment found in function compute_log_dist. It can be called for any sample if we already\n",
    "                     have computed logZ. In the aforementioned function we already have computed observation matrix oMat\n",
    "                     which is X dotProd W, so it was handy to use it to compute log P(y|X) in place; here we need to re\n",
    "                     compute oMat. NOTE: Not currently used.\n",
    "    \"\"\"\n",
    "    logPy_X = 0\n",
    "    oMat = np.matmul(seq,w)\n",
    "    # Select the column that is the confidence result for the specifc, known true laabel only. Do this for all rows of seq (each datum)\n",
    "    wSum, tSum = np.sum(oMat[np.arange(len(labels)),labels-1]), np.sum(t[labels[0:-1]-1, labels[1:]-1])\n",
    "    return (wSum+tSum) - logZ\n",
    "# -----------------------------------------------------------------------------------\n",
    "def write_grads_to_file(gradW, gradT, fPath, label='gradient.txt'):\n",
    "    \n",
    "    os.makedirs(fPath, exist_ok=True) \n",
    "    filePath = os.path.join(fPath, label)\n",
    "    try:\n",
    "        with open(filePath,'w') as f:\n",
    "            np.savetxt(f,gradW.reshape(-1,1))  # gradW is of shape 26x128: labels x features\n",
    "            np.savetxt(f,gradT.reshape(-1,1))  # gradT is of shape 26x26   labels x labels\n",
    "        return 1\n",
    "    except:\n",
    "        print(\"Failed to write gradients to file: \"+ filePath)\n",
    "        return 0\n",
    "        \n",
    "# -----------------------------------------------------------------------------------\n",
    "def compute_dataset_stats_n_grads(inData, inW, inT, inWordIdxs = None, filePath = None, label = 'gradient.txt'):\n",
    "    # Compute Avrage Gradient and log P(y|X)\n",
    "    wordEnd, sumLogPy_X = 0,0,\n",
    "    numOfLabels = inW.shape[1]\n",
    "    featSize = inW.shape[0]\n",
    "    avgGradW, avgGradT = np.zeros((numOfLabels, featSize)), np.zeros((numOfLabels, numOfLabels))\n",
    "    firstWord = 1 # if something needs debugging just enter the startword here, All previous words will be skipped\n",
    "    inWordIdxs = inWordIdxs if inWordIdxs is not None else extract_word_idxs(inData[:,1])\n",
    "    wordEnd = inWordIdxs[firstWord-1]\n",
    "\n",
    "    for i,w in enumerate(inWordIdxs[firstWord:]):\n",
    "        if i %500 == 0:\n",
    "            print(\"Computed Grads up to word: \" + str(i))\n",
    "        wordStart = wordEnd\n",
    "        wordEnd = w\n",
    "        logPY, logPYY, Z, logPy_X = compute_log_dist(inData[wordStart:wordEnd,2:], inW, inT, labels = inData[wordStart:wordEnd,0])\n",
    "        sumLogPy_X += logPy_X # It is already normalized by Z from the above function\n",
    "        gradW, gradT = compute_word_grads(inData[wordStart:wordEnd,2:],inData[wordStart:wordEnd,0], logPY, logPYY)\n",
    "        avgGradW += gradW\n",
    "        avgGradT += gradT\n",
    "\n",
    "    # Average out\n",
    "    numOfWords = wordIdxs.shape[0]\n",
    "    sumLogPy_X /= numOfWords\n",
    "    avgGradW /= numOfWords\n",
    "    avgGradT /= numOfWords\n",
    "    # save  Grad Results\n",
    "    if filePath is not None:\n",
    "        writeRs = write_grads_to_file(avgGradW, avgGradT, filePath, label = label)\n",
    "        \n",
    "    return sumLogPy_X, avgGradW, avgGradT\n",
    "# -------------------------------------------------------------------------------------------\n",
    "print(\"Computting average gradients for file {}. Data size: {}\".format(crfTrainFile,trainD.shape))\n",
    "#gradW, gradT = compute_dataset_stats_n_grads(lblTrainD, inW, inT, filePath = dataPath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.66603129335792"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(inT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Learn Weights!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_crf_obj(inData, w, t, c, savePath = None):\n",
    "    \"\"\" DESCIRPTION: COmpute the regularized CRF objective. Also returns the gradients to opti computation.\n",
    "    \"\"\"\n",
    "    crfObj = 0\n",
    "    #inWordIdxs = extract_word_idxs(inData[:,1])\n",
    "    label = 'solution.txt'\n",
    "    avgLogPy_X, gradW, gradY = compute_dataset_stats_n_grads(inData[:,:], w,t, filePath = savePath, label = label)\n",
    "    # Compute regulirized obbjective\n",
    "    crfObj = ( -c * avgLogPy_X )+ (0.5 *np.sum(np.power(w,2))) + ( 0.5 * np.sum(np.power(t,2)) )\n",
    "    return crfObj, gradW, gradT\n",
    "# ----------------------------------------------------------------------------------------\n",
    "def crf_obj(x, word_list, c, savePath = None):\n",
    "    \"\"\"Compute the CRF objective and gradient on the list of words (word_list)\n",
    "    evaluated at the current model x (w_y and T, stored as a vector)\n",
    "    \"\"\"\n",
    "    \n",
    "    # x is a vector as required by the solver. So reshape it to w_y and T\n",
    "    W = np.reshape(x[:128*26], (128, 26))  # each column of W is w_y (128 dim)\n",
    "    T = np.reshape(x[128*26:], (26, 26))  # T is 26*26\n",
    "\n",
    "    f, gradW, gradT = get_crf_obj(word_list, W, T, c, savePath=savePath)  # Compute the objective value of CRF\n",
    "                                         # objective log-likelihood + regularizer\n",
    "\n",
    "    g_W = gradW                  # compute the gradient in W(128 * 26)\n",
    "    g_T = gradT                  # compute the gradient in T(26*26)\n",
    "    g = np.concatenate([g_W.reshape(-1), g_T.reshape(-1)])  # Flatten the\n",
    "                                                          # gradient back into\n",
    "                                                          # a vector\n",
    "    return [f,g]\n",
    "\n",
    "c = 1000\n",
    "lblTrainD, lblTestD = append_label_to_crf_data(trainD, testD, sanTrainLabels, sanTestLabels)\n",
    "#f, gradW, gradT = get_crf_obj(lblTrainD[:,:], inW, inT, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3 Benchmarking with SVM!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data For SVMs and CRF loaded From part 2.a!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM HMM Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing:  /home/nikolaos/Workspaces/AML/Assignment_1_Programming/code/svm_hmm_linux64/svm_hmm_learn -c 1 /home/nikolaos/Workspaces/AML/Assignment_1_Programming/code/../data/train_struct.txt /home/nikolaos/Workspaces/AML/Assignment_1_Programming/code/models/svm_hmm_1.model\n",
      "Executing:  /home/nikolaos/Workspaces/AML/Assignment_1_Programming/code/svm_hmm_linux64/svm_hmm_classify /home/nikolaos/Workspaces/AML/Assignment_1_Programming/code/../data/test_struct.txt /home/nikolaos/Workspaces/AML/Assignment_1_Programming/code/models/svm_hmm_1.model /home/nikolaos/Workspaces/AML/Assignment_1_Programming/code/results/test_c_1.outtags\n",
      "0 0\n",
      "26189\n",
      "17079\n",
      "Executing:  /home/nikolaos/Workspaces/AML/Assignment_1_Programming/code/svm_hmm_linux64/svm_hmm_learn -c 2 /home/nikolaos/Workspaces/AML/Assignment_1_Programming/code/../data/train_struct.txt /home/nikolaos/Workspaces/AML/Assignment_1_Programming/code/models/svm_hmm_2.model\n",
      "Executing:  /home/nikolaos/Workspaces/AML/Assignment_1_Programming/code/svm_hmm_linux64/svm_hmm_classify /home/nikolaos/Workspaces/AML/Assignment_1_Programming/code/../data/test_struct.txt /home/nikolaos/Workspaces/AML/Assignment_1_Programming/code/models/svm_hmm_2.model /home/nikolaos/Workspaces/AML/Assignment_1_Programming/code/results/test_c_2.outtags\n",
      "0 0\n",
      "26189\n",
      "17801\n",
      "Executing:  /home/nikolaos/Workspaces/AML/Assignment_1_Programming/code/svm_hmm_linux64/svm_hmm_learn -c 5 /home/nikolaos/Workspaces/AML/Assignment_1_Programming/code/../data/train_struct.txt /home/nikolaos/Workspaces/AML/Assignment_1_Programming/code/models/svm_hmm_5.model\n",
      "Executing:  /home/nikolaos/Workspaces/AML/Assignment_1_Programming/code/svm_hmm_linux64/svm_hmm_classify /home/nikolaos/Workspaces/AML/Assignment_1_Programming/code/../data/test_struct.txt /home/nikolaos/Workspaces/AML/Assignment_1_Programming/code/models/svm_hmm_5.model /home/nikolaos/Workspaces/AML/Assignment_1_Programming/code/results/test_c_5.outtags\n",
      "0 0\n",
      "26189\n",
      "18872\n",
      "Executing:  /home/nikolaos/Workspaces/AML/Assignment_1_Programming/code/svm_hmm_linux64/svm_hmm_learn -c 10 /home/nikolaos/Workspaces/AML/Assignment_1_Programming/code/../data/train_struct.txt /home/nikolaos/Workspaces/AML/Assignment_1_Programming/code/models/svm_hmm_10.model\n",
      "Executing:  /home/nikolaos/Workspaces/AML/Assignment_1_Programming/code/svm_hmm_linux64/svm_hmm_classify /home/nikolaos/Workspaces/AML/Assignment_1_Programming/code/../data/test_struct.txt /home/nikolaos/Workspaces/AML/Assignment_1_Programming/code/models/svm_hmm_10.model /home/nikolaos/Workspaces/AML/Assignment_1_Programming/code/results/test_c_10.outtags\n",
      "0 0\n",
      "26189\n",
      "19651\n",
      "Executing:  /home/nikolaos/Workspaces/AML/Assignment_1_Programming/code/svm_hmm_linux64/svm_hmm_learn -c 100 /home/nikolaos/Workspaces/AML/Assignment_1_Programming/code/../data/train_struct.txt /home/nikolaos/Workspaces/AML/Assignment_1_Programming/code/models/svm_hmm_100.model\n",
      "Executing:  /home/nikolaos/Workspaces/AML/Assignment_1_Programming/code/svm_hmm_linux64/svm_hmm_classify /home/nikolaos/Workspaces/AML/Assignment_1_Programming/code/../data/test_struct.txt /home/nikolaos/Workspaces/AML/Assignment_1_Programming/code/models/svm_hmm_100.model /home/nikolaos/Workspaces/AML/Assignment_1_Programming/code/results/test_c_100.outtags\n",
      "0 0\n",
      "26189\n",
      "21564\n"
     ]
    }
   ],
   "source": [
    "# C parameter for regulirization impact.\n",
    "cParams = [1,2,5,10,100]\n",
    "# array that holds accuracies for all plots. its going to be 2x3x|cParmas| = letter, word-wise accuracy x 3 models x number of examined c Params\n",
    "# 1 dim: 0=letter-wise acc, 1 = word-wise acc\n",
    "# 2 dim: 0=CRF, 1=SVM HMM, 2 = SVM-MC\n",
    "# 3 dim: the various c Params\n",
    "acc = np.zeros((2,3,len(cParams)))\n",
    "for i,c in enumerate(cParams):\n",
    "    learnOptions = '-c ' +str(c)\n",
    "    svmHmmOutput = os.path.join(dir_path,'models/svm_hmm_'+str(c)+'.model')\n",
    "    ## Create a command to call the binaries required\n",
    "    cmd = ' '.join((learnerPath, learnOptions, svmTrainData, svmHmmOutput)) # ./svm_hmm_learn -c 5 -e 0.5 example7/declaration_of_independence.dat declaration.model \n",
    "    outFile = os.path.join(dir_path,'results/test_c_'+str(c)+'.outtags' )\n",
    "    cmdTest = ' '.join((testerPath, svmTestData, svmHmmOutput, outFile)) \n",
    "    #y, x = lbl.svm_read_problem(svmTrainData, return_scipy = True) # y: ndarray, x: csr_matrix \n",
    "    #m = lbl.train(y[:200], x[:200, :], '-c 4')\n",
    "    print(\"Executing: \" ,cmd)\n",
    "    exit = os.system(cmd) # returns the exit status\n",
    "    print(\"Executing: \" ,cmdTest)\n",
    "    exit2 = os.system(cmdTest) # returns the exit status\n",
    "    print(exit, exit2)\n",
    "    labels = get_svm_hmm_results(outFile)\n",
    "    letterAcc, wordAcc = evaluate_predictions(np.asarray(labels,dtype=np.int), sanTestLabels[:,0], wordTestIdxs)\n",
    "    acc[0,1,i], acc[1,1,i] = letterAcc, wordAcc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SV-MC Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing LIBLINEAR Multi Classification for C: 1 \n",
      "Accuracy = 69.3488% (18168/26198) (classification)\n",
      "26189\n",
      "18159\n",
      "Executing LIBLINEAR Multi Classification for C: 2 \n",
      "Accuracy = 69.2992% (18155/26198) (classification)\n",
      "26189\n",
      "18146\n",
      "Executing LIBLINEAR Multi Classification for C: 5 \n",
      "Accuracy = 69.3106% (18158/26198) (classification)\n",
      "26189\n",
      "18149\n",
      "Executing LIBLINEAR Multi Classification for C: 10 \n",
      "Accuracy = 68.3907% (17917/26198) (classification)\n",
      "26189\n",
      "17910\n",
      "Executing LIBLINEAR Multi Classification for C: 100 \n",
      "Accuracy = 54.2446% (14211/26198) (classification)\n",
      "26189\n",
      "14207\n"
     ]
    }
   ],
   "source": [
    "# LIBLINEAR SVM for character by character multi-classification!\n",
    "# Read the data!\n",
    "yTrain, xTrain = lbl.svm_read_problem(libSVMTrainData, return_scipy = True) # y: ndarray, x: csr_matrix\n",
    "yTest, xTest = lbl.svm_read_problem(libSVMTestData, return_scipy = True) # y: ndarray, x: csr_matrix\n",
    "for i,c in enumerate(cParams):\n",
    "    learnOptions = '-c ' +str(c)\n",
    "    outFile = 'results/liblinear-mc_c_'+str(c)+'.outtags' \n",
    "    print(\"Executing LIBLINEAR Multi Classification for C: {} \".format(c))\n",
    "    model   = lbl.train(yTrain, xTrain, learnOptions)\n",
    "    p_label, p_acc, p_val = lbl.predict(yTest, xTest, model)\n",
    "    letterAcc, wordAcc = evaluate_predictions(np.asarray(p_label,dtype=np.int), sanTestLabels[:,0], wordTestIdxs)\n",
    "    acc[0,2,i] = p_acc[0]\n",
    "    acc[1,2,i] = wordAcc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(128, 26)"
      ]
     },
     "execution_count": 302,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc[0,2,:]\n",
    "inW.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CRF Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training CRF ... c = 1000 \n",
      "\n",
      "3438\n",
      "Computed Grads up to word: 0\n",
      "Computed Grads up to word: 500\n",
      "Computed Grads up to word: 1000\n",
      "Computed Grads up to word: 1500\n",
      "Computed Grads up to word: 2000\n",
      "Computed Grads up to word: 2500\n",
      "Computed Grads up to word: 3000\n",
      "0.0 (4004,)\n",
      "3438\n",
      "Computed Grads up to word: 0\n",
      "Computed Grads up to word: 500\n",
      "Computed Grads up to word: 1000\n",
      "Computed Grads up to word: 1500\n",
      "Computed Grads up to word: 2000\n",
      "Computed Grads up to word: 2500\n",
      "Computed Grads up to word: 3000\n",
      "6.715245278179355e-15 (4004,)\n",
      "3438\n",
      "Computed Grads up to word: 0\n",
      "Computed Grads up to word: 500\n",
      "Computed Grads up to word: 1000\n",
      "Computed Grads up to word: 1500\n",
      "Computed Grads up to word: 2000\n",
      "Computed Grads up to word: 2500\n",
      "Computed Grads up to word: 3000\n",
      "30.242776132510027 (4004,)\n",
      "3438\n",
      "Computed Grads up to word: 0\n",
      "Computed Grads up to word: 500\n",
      "Computed Grads up to word: 1000\n",
      "Computed Grads up to word: 1500\n",
      "Computed Grads up to word: 2000\n",
      "Computed Grads up to word: 2500\n",
      "Computed Grads up to word: 3000\n",
      "7.560694033127507 (4004,)\n",
      "3438\n",
      "Computed Grads up to word: 0\n",
      "Computed Grads up to word: 500\n",
      "Computed Grads up to word: 1000\n",
      "Computed Grads up to word: 1500\n",
      "Computed Grads up to word: 2000\n",
      "Computed Grads up to word: 2500\n",
      "Computed Grads up to word: 3000\n",
      "1.8901735082818767 (4004,)\n",
      "3438\n",
      "Computed Grads up to word: 0\n",
      "Computed Grads up to word: 500\n",
      "Computed Grads up to word: 1000\n",
      "Computed Grads up to word: 1500\n",
      "Computed Grads up to word: 2000\n",
      "Computed Grads up to word: 2500\n",
      "Computed Grads up to word: 3000\n",
      "0.4725433770704692 (4004,)\n",
      "3438\n",
      "Computed Grads up to word: 0\n",
      "Computed Grads up to word: 500\n",
      "Computed Grads up to word: 1000\n",
      "Computed Grads up to word: 1500\n",
      "Computed Grads up to word: 2000\n",
      "Computed Grads up to word: 2500\n",
      "Computed Grads up to word: 3000\n",
      "0.1181358442676173 (4004,)\n",
      "3438\n",
      "Computed Grads up to word: 0\n",
      "Computed Grads up to word: 500\n",
      "Computed Grads up to word: 1000\n",
      "Computed Grads up to word: 1500\n",
      "Computed Grads up to word: 2000\n",
      "Computed Grads up to word: 2500\n",
      "Computed Grads up to word: 3000\n",
      "0.029533961066904323 (4004,)\n",
      "3438\n",
      "Computed Grads up to word: 0\n",
      "Computed Grads up to word: 500\n",
      "Computed Grads up to word: 1000\n",
      "Computed Grads up to word: 1500\n",
      "Computed Grads up to word: 2000\n",
      "Computed Grads up to word: 2500\n",
      "Computed Grads up to word: 3000\n",
      "0.007383490266726081 (4004,)\n",
      "3438\n",
      "Computed Grads up to word: 0\n",
      "Computed Grads up to word: 500\n",
      "Computed Grads up to word: 1000\n",
      "Computed Grads up to word: 1500\n",
      "Computed Grads up to word: 2000\n",
      "Computed Grads up to word: 2500\n",
      "Computed Grads up to word: 3000\n",
      "0.0018458725666815202 (4004,)\n",
      "3438\n",
      "Computed Grads up to word: 0\n",
      "Computed Grads up to word: 500\n",
      "Computed Grads up to word: 1000\n",
      "Computed Grads up to word: 1500\n",
      "Computed Grads up to word: 2000\n",
      "Computed Grads up to word: 2500\n",
      "Computed Grads up to word: 3000\n",
      "0.00046146814167038005 (4004,)\n",
      "3438\n",
      "Computed Grads up to word: 0\n",
      "Computed Grads up to word: 500\n",
      "Computed Grads up to word: 1000\n",
      "Computed Grads up to word: 1500\n",
      "Computed Grads up to word: 2000\n",
      "Computed Grads up to word: 2500\n",
      "Computed Grads up to word: 3000\n",
      "0.00011536703541759501 (4004,)\n",
      "3438\n",
      "Computed Grads up to word: 0\n",
      "Computed Grads up to word: 500\n",
      "Computed Grads up to word: 1000\n",
      "Computed Grads up to word: 1500\n",
      "Computed Grads up to word: 2000\n",
      "Computed Grads up to word: 2500\n",
      "Computed Grads up to word: 3000\n",
      "2.8841758854398753e-05 (4004,)\n",
      "3438\n",
      "Computed Grads up to word: 0\n",
      "Computed Grads up to word: 500\n",
      "Computed Grads up to word: 1000\n",
      "Computed Grads up to word: 1500\n",
      "Computed Grads up to word: 2000\n",
      "Computed Grads up to word: 2500\n",
      "Computed Grads up to word: 3000\n",
      "7.210439713599688e-06 (4004,)\n",
      "3438\n",
      "Computed Grads up to word: 0\n",
      "Computed Grads up to word: 500\n",
      "Computed Grads up to word: 1000\n",
      "Computed Grads up to word: 1500\n",
      "Computed Grads up to word: 2000\n",
      "Computed Grads up to word: 2500\n",
      "Computed Grads up to word: 3000\n",
      "1.802609928399922e-06 (4004,)\n",
      "3438\n",
      "Computed Grads up to word: 0\n",
      "Computed Grads up to word: 500\n",
      "Computed Grads up to word: 1000\n",
      "Computed Grads up to word: 1500\n",
      "Computed Grads up to word: 2000\n",
      "Computed Grads up to word: 2500\n",
      "Computed Grads up to word: 3000\n",
      "4.506524820999805e-07 (4004,)\n",
      "3438\n",
      "Computed Grads up to word: 0\n",
      "Computed Grads up to word: 500\n",
      "Computed Grads up to word: 1000\n",
      "Computed Grads up to word: 1500\n",
      "Computed Grads up to word: 2000\n",
      "Computed Grads up to word: 2500\n",
      "Computed Grads up to word: 3000\n",
      "1.1266312052499513e-07 (4004,)\n",
      "3438\n",
      "Computed Grads up to word: 0\n",
      "Computed Grads up to word: 500\n",
      "Computed Grads up to word: 1000\n",
      "Computed Grads up to word: 1500\n",
      "Computed Grads up to word: 2000\n",
      "Computed Grads up to word: 2500\n",
      "Computed Grads up to word: 3000\n",
      "2.8165780131248782e-08 (4004,)\n",
      "3438\n",
      "Computed Grads up to word: 0\n",
      "Computed Grads up to word: 500\n",
      "Computed Grads up to word: 1000\n",
      "Computed Grads up to word: 1500\n",
      "Computed Grads up to word: 2000\n"
     ]
    }
   ],
   "source": [
    "def crf_train(inTrainD, c, saveModelFile = None):\n",
    "    print('Training CRF ... c = {} \\n'.format(c))\n",
    "\n",
    "    # Initial value of the parameters W and T, stored in a vector\n",
    "    x0 = np.zeros((128*26+26**2,1))\n",
    "\n",
    "    # Start the optimization\n",
    "    result = opt.fmin_tnc(crf_obj, x0, args = [inTrainD, c], maxfun=100,\n",
    "                          ftol=1e-3, disp=5)\n",
    "    model  = result[0]\n",
    "    if saveModelFile is not None:\n",
    "        with open(saveModelFile, 'w') as f:\n",
    "            np.savetxt(f, model)\n",
    "    W = np.reshape(model[:128*26], (128, 26))  # each column of W is w_y (128 dim)\n",
    "    T = np.reshape(model[128*26:], (26, 26))  # T is 26*26\n",
    "    \n",
    "    return W, T\n",
    "    \n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "def crf_test(inTest, w,t, c, labels=None, inWordIdxs = None,outFile = False):\n",
    "    \n",
    "    \"\"\" DESCRIPTION: Thisfunction will decote input sequence, produce leeter and word sie acc score and optionally write the\n",
    "                     resulting decoded sequence to a file!\n",
    "        ARGUMENTS: outFile (File Path): The file to hold the decoded sequences.\n",
    "        RETURNS: letterAcc (scalar) Letter-wise accuracy.\n",
    "                 wordACC (scalar): Word-Wise accuracy.\n",
    "    \"\"\"\n",
    "    inWordIdxs = inWordIdxs if inWordIdxs is not None else extract_word_idxs(inData[:,1])\n",
    "    testLetterLabels = labels if labels is not None else inTest[:,0]\n",
    "    dataStart = 0 \n",
    "    if labels is not None:\n",
    "        dataStart += 1\n",
    "    if inWordIdxs is not None:\n",
    "        dataStart += 1\n",
    "    letterAcc, wordAcc = 0, 0\n",
    "    firstWord = 1 # if something needs debugging just enter the startword here, All previous words will be skipped\n",
    "    wordEnd = inWordIdxs[firstWord-1]\n",
    "    # Decode each word\n",
    "    for i, limit in enumerate(inWordIdxs[firstWord:]):\n",
    "        if i %500 == 0:\n",
    "            print(\"Decoded up to word: \" + str(i))\n",
    "        wordStart = wordEnd\n",
    "        wordEnd = limit\n",
    "        \n",
    "        decRes, maxObj = crf_decode(testD[wordStart:wordEnd, dataStart], w, t)\n",
    "        # Get correct labels\n",
    "        res = np.sum((decRes == testLetterLabels[wordStart:wordEnd]).astype(int))     # find the number of labels that are equal to ground Truth\n",
    "        letterAcc += res                                                              # Letterwise acc is increases for every match found\n",
    "        wordAcc += 1 if res == decRes.shape[0] else 0                                 # word acc increases only when ALL labels are correct\n",
    "    # Average out letter-wise acc over all letter and word-wise over all words.    \n",
    "    letterAcc /= trainD.shape[0]\n",
    "    wordAcc /= testWordIdx.shape[0]\n",
    "    \n",
    "    return letterAcc, wordAcc\n",
    "# ---------------------------------------------------------------------------\n",
    "def evaluate_crf(inTrainD, inTestD, saveModelFile = None, saveAccFile = None):\n",
    "    \n",
    "    \"\"\" DESCRIPTION: Thisfunction will decote input sequence, produce leeter and word sie acc score and optionally write the\n",
    "                     resulting decoded sequence to a file!\n",
    "        ARGUMENTS: outFile (File Path): The file to hold the decoded sequences.\n",
    "        RETURNS: letterAcc (scalar) Letter-wise accuracy.\n",
    "                 wordACC (scalar): Word-Wise accuracy.\n",
    "    \"\"\"\n",
    "    # Train Part\n",
    "    savePath = saveModelFile\n",
    "    w, t = crf_train(inTrainD, c, saveModelFile = saveModelFile)\n",
    "    \n",
    "    # Test Part\n",
    "    letterAcc, wordAcc = crf_test(inTestD, w,t ,c)   \n",
    "    print('CRF test accuracy for c = {}\\nLetter-wise {}\\nWord-Wise'.format(c, letterAcc, wordAcc))\n",
    "    return letterAcc, wordAcc\n",
    "# -------------------------------------------------------------\n",
    "saveModelFile = os.path.join(dir_path, 'models')\n",
    "saveAccFile = os.path.join(resPath, 'trained_crf_c_1000.txt')\n",
    "evaluate_crf(lblTrainD, lblTestD, saveModelFile = saveModelFile, saveAccFile = saveAccFile)\n",
    "print(letterAcc, wordAcc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Error Computation and Plot Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_svm_hmm_results(predFile, gTruth = None):\n",
    "    acc = 0\n",
    "    #if isinstance(type(gTruth), typedir) and isinstance(type(pred),dir):\n",
    "    if gTruth is None:\n",
    "        gTruth = '../data/sanitizedTestTruth.txt'\n",
    "    elif isinstance(gTruth, np.ndarray):\n",
    "        trueClass = gTruth\n",
    "    else:\n",
    "        with open(gTruth, 'r') as g:\n",
    "            trueClass = np.asarray([x.split(' ')[0] for x in g.readlines()]) # keep in mind that readlines retunrs the \\n character\n",
    "            \n",
    "    with open(predFile, 'r') as p:\n",
    "        predClass = np.asarray(p.read().splitlines())\n",
    "        \n",
    "    #acc = np.sum(predClass == trueClass) / trueClass.shape[0]\n",
    "    return predClass\n",
    "# -------------------------------------\n",
    "def plot_accuracy(cParams, accs, labels= None):\n",
    "    \"\"\" DESCRIPTION: THis function will produce pltos for letter wands wordwise accuracy as a function of C params.\n",
    "                     It expects ndArrays.   \n",
    "        ARGUMENTS: xAxis(ndarray): values of C parameters.\n",
    "                   yAxis(ndArray): acc values for the various models\n",
    "    \"\"\"\n",
    "    numOfPlots = 1\n",
    "    if accs.ndim >1:\n",
    "        numOfPlots = accs.shape[0] # ist dim has how many models exist!\n",
    "    plt.figure(figsize=(6.5*numOfPlots, 5))\n",
    "    for n in range(numOfPlots):#\n",
    "        print(n)\n",
    "        ax=plt.subplot(int('1'+str(numOfPlots)+str(n+1))) #index cannot be 0. As such increas it by 1 \n",
    "        if labels is not None and len(labels) >= n:\n",
    "            ax.set_title(labels[n])\n",
    "        if accs.ndim <2:\n",
    "            plt.plot(cParams, accs[:],marker = 'x')\n",
    "            for x,y in zip(cParams, accs):                                       # <--\n",
    "                plt.annotate('C: '+str(x), # this is the text\n",
    "                     (x,y), # this is the point to label\n",
    "                     textcoords=\"offset points\", # how to position the text\n",
    "                     xytext=(0,7), # distance from text to points (x,y)\n",
    "                     ha='center') # horizontal alignment can be left, right or center\n",
    "        else:\n",
    "            plt.plot(cParams, accs[n,:], marker = 'x')\n",
    "            for x,y in zip(cParams, accs[n,:]):                                       # <--\n",
    "                plt.annotate('C: '+str(x), # this is the text\n",
    "                     (x,y), # this is the point to label\n",
    "                     textcoords=\"offset points\", # how to position the text\n",
    "                     xytext=(0,7), # distance from text to points (x,y)\n",
    "                     ha='center') # horizontal alignment can be left, right or center\n",
    "        \n",
    "    plt.suptitle('Letterwise Accuracy vs C parameters')\n",
    "    plt.xlabel('C Parameters')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy results for SVM HMM, for C: [1, 2, 5, 10, 100]:\n",
      " [0.65191999 0.67947935 0.72036033 0.75009543 0.82311627]\n",
      "Accuracy results for SVM MC, for C: [1, 2, 5, 10, 100]:\n",
      " [0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Compute and print results\n",
    "# NOTE: acc matrix is dim1 : 0: letter Acc 1: word Acc | DIM2: 0=CRF, 1=SVM_HMM, 2= SVM_MC | DIM3: i for each of the c params\n",
    "#for i,c in enumerate(cParams):\n",
    "    #outFile = 'results/test_c_'+str(c)+'.outtags' \n",
    "    #acc[0,1,i] = compute_letterwise_accuracy(outFile, gTruth = gTruth)\n",
    "print(\"Accuracy results for SVM HMM, for C: {}:\\n {}\".format(cParams, acc[0,1,:]))\n",
    "print(\"Accuracy results for SVM MC, for C: {}:\\n {}\".format(cParams, acc[0,2,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 936x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_accuracy(cParams, acc[0,1:,:], labels = ['SVM-HMM', 'SVM-MC'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PART 4\n",
    "## The evil machine learner\n",
    "#### Load Indx Files, and Corruptions pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corrupted Data files not found. Building them From Scratch!\n",
      "Creating All train files (CRF, SVM-HMM, SVM-MC) with K=0 corrupted lines\n",
      "Writing: \n",
      "-> CRF train data to /home/nikolaos/Workspaces/AML/Assignment_1_Programming/code/../data/0corr_crf_train.txt\n",
      "-> SVM-MC train data to /home/nikolaos/Workspaces/AML/Assignment_1_Programming/code/../data/0corr_liblinear_svm_train.txt\n",
      "-> HMM Train data to /home/nikolaos/Workspaces/AML/Assignment_1_Programming/code/../data/0corr_svm_hmm_train.txt\n",
      "Creating All train files (CRF, SVM-HMM, SVM-MC) with K=500 corrupted lines\n",
      "Writing: \n",
      "-> CRF train data to /home/nikolaos/Workspaces/AML/Assignment_1_Programming/code/../data/500corr_crf_train.txt\n",
      "-> SVM-MC train data to /home/nikolaos/Workspaces/AML/Assignment_1_Programming/code/../data/500corr_liblinear_svm_train.txt\n",
      "-> HMM Train data to /home/nikolaos/Workspaces/AML/Assignment_1_Programming/code/../data/500corr_svm_hmm_train.txt\n",
      "Creating All train files (CRF, SVM-HMM, SVM-MC) with K=1000 corrupted lines\n",
      "Writing: \n",
      "-> CRF train data to /home/nikolaos/Workspaces/AML/Assignment_1_Programming/code/../data/1000corr_crf_train.txt\n",
      "-> SVM-MC train data to /home/nikolaos/Workspaces/AML/Assignment_1_Programming/code/../data/1000corr_liblinear_svm_train.txt\n",
      "-> HMM Train data to /home/nikolaos/Workspaces/AML/Assignment_1_Programming/code/../data/1000corr_svm_hmm_train.txt\n",
      "Creating All train files (CRF, SVM-HMM, SVM-MC) with K=1500 corrupted lines\n",
      "Writing: \n",
      "-> CRF train data to /home/nikolaos/Workspaces/AML/Assignment_1_Programming/code/../data/1500corr_crf_train.txt\n",
      "-> SVM-MC train data to /home/nikolaos/Workspaces/AML/Assignment_1_Programming/code/../data/1500corr_liblinear_svm_train.txt\n",
      "-> HMM Train data to /home/nikolaos/Workspaces/AML/Assignment_1_Programming/code/../data/1500corr_svm_hmm_train.txt\n",
      "Creating All train files (CRF, SVM-HMM, SVM-MC) with K=2000 corrupted lines\n",
      "Writing: \n",
      "-> CRF train data to /home/nikolaos/Workspaces/AML/Assignment_1_Programming/code/../data/2000corr_crf_train.txt\n",
      "-> SVM-MC train data to /home/nikolaos/Workspaces/AML/Assignment_1_Programming/code/../data/2000corr_liblinear_svm_train.txt\n",
      "-> HMM Train data to /home/nikolaos/Workspaces/AML/Assignment_1_Programming/code/../data/2000corr_svm_hmm_train.txt\n",
      "Creating All train files (CRF, SVM-HMM, SVM-MC) with K=2500 corrupted lines\n",
      "Writing: \n",
      "-> CRF train data to /home/nikolaos/Workspaces/AML/Assignment_1_Programming/code/../data/2500corr_crf_train.txt\n",
      "-> SVM-MC train data to /home/nikolaos/Workspaces/AML/Assignment_1_Programming/code/../data/2500corr_liblinear_svm_train.txt\n",
      "-> HMM Train data to /home/nikolaos/Workspaces/AML/Assignment_1_Programming/code/../data/2500corr_svm_hmm_train.txt\n"
     ]
    }
   ],
   "source": [
    "def transform_struct_to_matrix_file(inStructFile):\n",
    "    \"\"\" Not Needed\n",
    "    \"\"\"\n",
    "    outMatrixFile = os.path.join(inStructFile.rsplit('/',1)[0], inStructFile.rsplit('/',1)[1].split('.')[0] + '_matrix.txt')\n",
    "    m = 0\n",
    "    print(\"Writing conderted struct-> matrix output to {}\".format(outMatrixFile))\n",
    "       # with open(inStructFile,'r') as i, open(outMatrixFile,'w') as o:\n",
    "    return m\n",
    "\n",
    "# ---------------------------------------------------------------\n",
    "\n",
    "# define rotataion translation funtiocs\n",
    "def corrupt_data(data, wordIdxs, cScheme, k='all'):\n",
    "    \"\"\" DESCRIPTION: Apply the corruption transformation as specified in cScheme  to data, indexed by word IDXs.\n",
    "        ARGUMENTS: data(narray): Matrix holdsing the raw data; dims: numOfLetterx128\n",
    "                   wordIdx(nd array): Array holding the word each letter belogns to; dims numOfLetter x 1\n",
    "                   cScheme (list): A list holding the corruption transofrmations. Format is\n",
    "                       r wordIdx degrees                  for rotation \n",
    "                       t word idx xTrans yTrans(optional) for translation\n",
    "        RETURNS: corruptedData(nd array): The matrix holding the transformed data. DIMS: numOf letters x 128\n",
    "    \"\"\"\n",
    "    corruptedData = np.copy(data)\n",
    "    k = k if k is not 'all' else int(data.shape[0])\n",
    "    # For each pixel (x, y), the output will be calculated as (ax+by+c, dx+ey+f). \n",
    "    # So if you want to apply a translation, you only have to look at the c and f values of your matrix.\n",
    "    a = 1\n",
    "    b = 0\n",
    "    c = 0 #left/right (i.e. 5/-5)\n",
    "    d = 0\n",
    "    e = 1\n",
    "    f = 0 #up/down (i.e. 5/-5)\n",
    "    # Extract the corruption shceme\n",
    "    for i,s in enumerate(cScheme[0:k]):\n",
    "        tarWord = int(s[1])\n",
    "        j = 0\n",
    "        if s[0] == 'r': # for rotation\n",
    "            for d in data[wordIdxs[tarWord-1]: wordIdxs[tarWord],:]:    \n",
    "                d = d.reshape(16,8)\n",
    "                img = Image.fromarray(d.astype(np.uint8))\n",
    "                img = img.rotate(int(s[1])) \n",
    "                corruptedData[tarWord-1+j,:] = np.array(img).reshape(128)\n",
    "                j += 1\n",
    "        elif s[1] == 't': # for translation\n",
    "            xTrans = int(s[2])\n",
    "            yTrans = int(s[3]) if len(s) >3 else 0\n",
    "\n",
    "            for d in data[wordIdxs[tarWord-1]: wordIdxs[tarWord],:]:    \n",
    "                d = d.reshape(16,8)\n",
    "                img = Image.fromarray(d.astype(np.uint8))\n",
    "                img = img.rotate(s[1]) \n",
    "                img = img.transform(img.size, Image.AFFINE, (a, b, xTrans, d, e, yTrans))\n",
    "                corruptedData[tarWord-1+j,:] = np.array(img).reshape(128,-1)\n",
    "                j += 1\n",
    "    \n",
    "    return corruptedData\n",
    "# --------------------------------------------------------------------------------------------------------\n",
    "\n",
    "def save_matrix_as_structs(matrix, gTruth, savePath, k=0):\n",
    "    \"\"\" DESCRIPTION: This funciton takes the input data, along with letter-wise and word-wise labels and transforms\n",
    "                     them into a readable format for liblinear SVM and HMM SVM. It writes the transformed output into\n",
    "                     2 separate files.\n",
    "    \"\"\"\n",
    "    svmMcFile  = os.path.join(savePath, str(k)+'corr_liblinear_svm_train.txt')\n",
    "    svmHMMFile = os.path.join(savePath, str(k)+'corr_svm_hmm_train.txt')\n",
    "    crfFile    = os.path.join(savePath, str(k)+'corr_crf_train.txt') \n",
    "    # Turn and save as liblinear struct\n",
    "    line  = []\n",
    "    line2 = []\n",
    "    line3 = []\n",
    "    try:\n",
    "        print(\"Writing: \\n-> CRF train data to {}\\n-> SVM-MC train data to {}\\n-> HMM Train data to {}\".format(crfFile,svmMcFile, svmHMMFile))\n",
    "        with open(svmMcFile, 'w') as s, open(svmHMMFile, 'w') as h, open(crfFile,'w') as c:\n",
    "            for i, d in enumerate(matrix): \n",
    "                #print(gTruth[i,0])\n",
    "                line.append(str(gTruth[i,0])) # append label \n",
    "                line2.append(str(gTruth[i,0]) +\" qid:\"+str(gTruth[i,1])) # append label \n",
    "                for j, v in enumerate(d): # append all data. Only insert non-zero vals -> sparse representation!\n",
    "                    if v != 0:\n",
    "                        line.append(str(j)+\":\"+str(v)) \n",
    "                        line2.append(str(int(j))+\":\"+str(int(v))) \n",
    "                        #print(line)\n",
    "                line.append(\"\\n\") \n",
    "                line2.append(\"\\n\") \n",
    "                s.write(' '.join(line))\n",
    "                h.write(' '.join(line2))\n",
    "                line = []\n",
    "                line2 = []\n",
    "            filler = np.ones((matrix.shape[0],5)) * -1\n",
    "            np.savetxt(c, np.append(filler,matrix,axis=1), fmt = '%d')\n",
    "        return 1\n",
    "    except:\n",
    "        print(\"Failes to wrtie corrupted matrix data to files.\")\n",
    "        return 0\n",
    "# ---------------------------------------------------------------\n",
    "# Perform Train Data (Only) Tranformations - if not already done.\n",
    "# Load the corruption scheme\n",
    "cSchemeFile = os.path.join(dataPath, 'transform.txt')\n",
    "cDataFile = os.path.join(dataPath, '0corr_liblinear_svm_train.txt')\n",
    "corruptLines= [0,500,1000,1500,2000,2500]\n",
    "if not os.path.exists(cDataFile):\n",
    "    print(\"Corrupted Data files not found. Building them From Scratch!\")\n",
    "    with open(cSchemeFile,'r') as c, open(cDataFile, 'w') as d:\n",
    "        cScheme =[x.split(' ') for x in c.read().splitlines()]   # split file into a list of lists of tokens\n",
    "        # Load the data in CRF readable format\n",
    "        crfTrainFile = os.path.join(dataPath, 'train.txt')\n",
    "        crfTestFile  = os.path.join(dataPath, 'test.txt')\n",
    "        trainD, testD = load_crf_data(crfTrainFile, crfTestFile)\n",
    "        for k in corruptLines:\n",
    "            print(\"Creating All train files (CRF, SVM-HMM, SVM-MC) with K={} corrupted lines\".format(k))\n",
    "            corruptedData = corrupt_data(trainD, wordIdxs, cScheme, k =  k)\n",
    "            save_matrix_as_structs(corruptedData, sanTrainLabels, dataPath, k =k)\n",
    "       \n",
    "# ---------------------------------------------------------------\n",
    "# check if values are already loaded\n",
    "try:\n",
    "    corrTrainD\n",
    "except NameError:\n",
    "        # Load the data in CRF readable format\n",
    "        crfTrainFile = os.path.join(dataPath, 'train.txt')\n",
    "        crfTestFile  = os.path.join(dataPath, 'test.txt')\n",
    "        trainD, testD = load_crf_data(crfTrainFile, crfTestFile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate Models on different K= [0:500:2500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/nikolaos/Workspaces/AML/Assignment_1_Programming/code/../data'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating SVM HMM at c:7 with k= 0 Corrupted lines\n",
      "Executing:  /home/nikolaos/Workspaces/AML/Assignment_1_Programming/code/svm_hmm_linux64/svm_hmm_learn -c 7 /home/nikolaos/Workspaces/AML/Assignment_1_Programming/code/../data/0corr_svm_hmm_train.txt /home/nikolaos/Workspaces/AML/Assignment_1_Programming/code/models/svm_hmm_c_7.model\n",
      "Write results to  /home/nikolaos/Workspaces/AML/Assignment_1_Programming/code/results/test_c_7.outtags\n",
      "Executing /home/nikolaos/Workspaces/AML/Assignment_1_Programming/code/svm_hmm_linux64/svm_hmm_classify /home/nikolaos/Workspaces/AML/Assignment_1_Programming/code/../data/test_struct.txt /home/nikolaos/Workspaces/AML/Assignment_1_Programming/code/models/svm_hmm_c_7.model /home/nikolaos/Workspaces/AML/Assignment_1_Programming/code/results/test_c_7.outtags\n",
      "256 0\n",
      "0.0\n",
      "\n",
      "Evaluating SVM MC at c:7 with k= 0 Corrupted lines\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/nikolaos/.virtualenvs/nlp/lib/python3.6/site-packages/ipykernel_launcher.py:15: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 67.5548% (17698/26198) (classification)\n",
      "(67.55477517367738, 38.38537292923124, 0.3501313878271823)\n"
     ]
    }
   ],
   "source": [
    "def SVM_HMM_eval(trainD, testD, learnerPath, testerPath, gTruth, c = 10, label =''): \n",
    "    learnOptions = '-c ' +str(c)\n",
    "    svmHmmOutput = os.path.join(dir_path,'models/svm_hmm_c_'+str(c)+label+'.model')\n",
    "    ## Create a command to call the binaries required\n",
    "    cmd1 = ' '.join((learnerPath, learnOptions, trainD, svmHmmOutput)) # ./svm_hmm_learn -c 5 -e 0.5 example7/declaration_of_independence.dat declaration.model \n",
    "    outFile = os.path.join(dir_path,'results/test_c_'+str(c)+label+'.outtags')\n",
    "    cmdTest1 = ' '.join((testerPath, testD, svmHmmOutput, outFile)) \n",
    "    #y, x = lbl.svm_read_problem(svmTrainData, return_scipy = True) # y: ndarray, x: csr_matrix \n",
    "    #m = lbl.train(y[:200], x[:200, :], '-c 4')\n",
    "    print(\"Executing: \" ,cmd1)\n",
    "    exit = os.system(cmd1) # returns the exit status\n",
    "    print(\"Write results to \", outFile)\n",
    "    print('Executing', cmdTest1)\n",
    "    exit2 = os.system(cmdTest1) # returns the exit status\n",
    "    print(exit, exit2)\n",
    "    ret = compute_letterwise_accuracy(outFile, gTruth = gTruth[:,0])\n",
    "    print(ret)\n",
    "    return ret\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "def SVM_MC_eval(trainFile, testFile, c = 10, label =''):\n",
    "    \"\"\" DESCRIPTION: This function will train a model of the data contained in trainFile and test omn the data in testFile.\n",
    "                     It will call upon liblinear's implementation of SVM to do multiclass (MC) chgaracter-wise classification.\n",
    "                     It will return the letterwise accuracy.\n",
    "    \"\"\"\n",
    "    wordAcc = 0\n",
    "    yTrain, xTrain = lbl.svm_read_problem(trainFile, return_scipy = True) # y: ndarray, x: csr_matrix\n",
    "    yTest, xTest = lbl.svm_read_problem(testFile, return_scipy = True) # y: ndarray, x: csr_matrix\n",
    "    learnOptions = '-c ' +str(c)\n",
    "    outFile = 'results/liblinear-mc_c_'+str(c)+label+'.outtags' \n",
    "    model   = lbl.train(yTrain, xTrain, learnOptions)\n",
    "    p_label, p_acc, p_val = lbl.predict(yTest, xTest, model)\n",
    "    print(p_acc)\n",
    "    #wordAcc = compute_wordwise_accuracy(p_label, wordIdxs)\n",
    "    return p_acc[0], wordAcc, p_label   \n",
    "# ------------------------------------------------------------------------\n",
    "cOpt = int(7)\n",
    "accCorr = np.zeros((2,3,len(corruptLines)))\n",
    "for i,k in enumerate(corruptLines[0:1]):\n",
    "    print(\"Evaluating SVM HMM at c:{} with k= {} Corrupted lines\".format(cOpt, k))\n",
    "    svmMcFile  = os.path.join(dataPath, str(k)+'corr_liblinear_svm_train.txt')\n",
    "    svmHMMTrainFile = os.path.join(dataPath, str(k)+'corr_svm_hmm_train.txt')\n",
    "    svmHMMTestFile = os.path.join(dataPath, 'test_struct.txt')\n",
    "    learnOptions = '-c ' +str(cOpt)\n",
    "    label = '_k_'+str(k)\n",
    "    #SVM_HMM_eval(svmHMMTrainFile, svmHMMTestFile, learnerPath, testerPath, sanTestLabels[:,0], c=cOpt, label = label)\n",
    "    #SVM_HMM_eval(svmHMMTrainFile, svmTestData, learnerPath, testerPath, sanTestLabels[:,0], c=cOpt, label = label)\n",
    "    accCorr[0, 1, i] = SVM_HMM_eval(svmHMMTrainFile, svmTestData,learnerPath, testerPath, sanTestLabels, c=cOpt)#, label='_k_'+str(k)+'m')\n",
    "    # Liblinear MC\n",
    "    print(\"\\nEvaluating SVM MC at c:{} with k= {} Corrupted lines\".format(cOpt, k))\n",
    "    p, w, l = SVM_MC_eval(svmMcFile, libSVMTestData, c = cOpt, label = label)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8500\n",
      "26189\n",
      "17689\n",
      "0.6752042140621421 0.01939745694757341\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------------------------------------------\n",
    "def evaluate_predictions(pred, letterLabels, wordIdxs):\n",
    "    letterAcc, wordAcc = 0,0\n",
    "    firstWord = 1\n",
    "    wordEnd = wordIdxs[firstWord-1]\n",
    "    for i, limit in enumerate(wordIdxs[firstWord:]):\n",
    "        #if i %500 == 0:\n",
    "        wordStart = wordEnd\n",
    "        wordEnd = limit\n",
    "        \n",
    "        decSeq = pred[wordStart:wordEnd]\n",
    "        # Get correct labels\n",
    "        res = np.sum((decSeq == letterLabels[wordStart:wordEnd]).astype(int))     # find the number of labels that are equal to ground Truth\n",
    "        letterAcc += res                                                              # Letterwise acc is increases for every match found\n",
    "        wordAcc += 1 if res == decSeq.shape[0] else 0                                 # word acc increases only when ALL labels are correct\n",
    "    # Average out letter-wise acc over all letter and word-wise over all words.    \n",
    "    print(wordEnd)\n",
    "    print(letterAcc)\n",
    "    letterAcc /= pred.shape[0]\n",
    "    wordAcc /= wordIdxs[-1]\n",
    "    return letterAcc, wordAcc\n",
    "# ---------------------------------\n",
    "print(np.sum(np.asarray(l) != sanTestLabels[:,0]).astype(int))\n",
    "letterAcc, wordAcc = evaluate_predictions(np.asarray(l,dtype=np.int), sanTestLabels[:,0], wordTestIdxs)\n",
    "print(letterAcc, wordAcc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
