{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/nikolaos/Workspaces/AML/Assignment_1_Programming/code/lib_liblinear/python', '/home/nikolaos/.virtualenvs/nlp/lib/python36.zip', '/home/nikolaos/.virtualenvs/nlp/lib/python3.6', '/home/nikolaos/.virtualenvs/nlp/lib/python3.6/lib-dynload', '/usr/lib/python3.6', '', '/home/nikolaos/.virtualenvs/nlp/lib/python3.6/site-packages', '/home/nikolaos/.virtualenvs/nlp/lib/python3.6/site-packages/IPython/extensions', '/home/nikolaos/.ipython']\n",
      "/home/nikolaos/Workspaces/AML/Assignment_1_Programming/code\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "import lib_liblinear.python.liblinearutil as lbl\n",
    "\n",
    "\n",
    "dir_path = Path.cwd()\n",
    "\n",
    "print(dir_path)\n",
    "sys.path.insert(0, dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done reading! Sizes are X: (12800,), W:(3328,),T: (676,)\n"
     ]
    }
   ],
   "source": [
    "dataPath = os.path.join(dir_path, '../data')\n",
    "letters = 'abcdefghijklmnopqrstuvwxyz'\n",
    "# Load decode Input\n",
    "decInputFile = os.path.join(dataPath, 'decode_input.txt')\n",
    "with open(decInputFile, 'r') as f:\n",
    "    inData = f.read().splitlines()\n",
    "    inX = np.asarray(inData[0:12800], dtype = np.float)     # input is 100 letters @ 128 = 12800, or 0-12799. NOTE: Recall Python slice is [).\n",
    "    inW = np.asarray(inData[12800:16128], dtype = np.float) # wieths are 26 * 128 = 3328, or 12800-16127\n",
    "    inT = np.asarray(inData[16128:], dtype = np.float)      # The remaing are the transition probs.\n",
    "print(\"Done reading! Sizes are X: {}, W:{},T: {}\".format(inX.shape, inW.shape, inT.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done reshaping! Sizes now are X: (100, 128), W:(128, 26),T: (26, 26)\n"
     ]
    }
   ],
   "source": [
    "# Reshape arrays\n",
    "# Reshape T to 26x26\n",
    "inT = np.reshape(inT,(26,26)) \n",
    "inX = np.reshape(inX, (-1,128))\n",
    "inW = np.reshape(inW, (128,-1))\n",
    "print(\"Done reshaping! Sizes now are X: {}, W:{},T: {}\".format(inX.shape, inW.shape, inT.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shapes: sequence: (100, 128), weights: (128, 26), T matrix:(26, 26)\n",
      "[7.78442958e-03 1.52115672e-02 1.20710706e-02 4.69876271e-03\n",
      " 5.34670451e-01 1.14755986e-02 5.64371856e-02 3.31816960e-02\n",
      " 4.00406494e-03 1.53669183e-01 4.88480308e-01 4.30666478e-05\n",
      " 3.11189330e-01 7.07851358e-02 6.51891449e-02 1.29705870e-01\n",
      " 1.68724845e-03 4.50357985e-01 6.23706302e-02 3.08509408e-03\n",
      " 2.69362883e-02 3.39592362e-01 2.16999483e-01 3.78132364e-02\n",
      " 1.30214979e-02 2.69906264e-01] (26,) (26,)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Max-Sum algorithm!\n",
    "def crf_decoder(seq, w, t):\n",
    "    # Init\n",
    "    seqLen  = seq.shape[0]\n",
    "    dataLen = w.shape[1]\n",
    "    cMat    = np.zeros((seqLen, dataLen))\n",
    "    bMat    = np.matmul(seq, w) # this gives the probability of observing each i standalone\n",
    "    maxIdx  = np.zeros((seqLen, dataLen), dtype = np.int)\n",
    "    decSeq  = []\n",
    "    ci = np.zeros((26,26))\n",
    "    \n",
    "    # Get step 0 estimates\n",
    "    step0    = np.matmul(seq[0,:], w)\n",
    "    c1 = np.multiply(step0,t)      # the previous node potential x edje potenital for all i->j pairs\n",
    "    maxIdx[0,:] = np.argmax(c1, axis = 0) #1x26 the max elem indexes\n",
    "    c1 = c1[maxIdx[0,:],range(c1.shape[1])] #1x26 select the max element, as indexed by maxIdx, for every column\n",
    "    cMat[0,:] = c1\n",
    "    print(\"Input shapes: sequence: {}, weights: {}, T matrix:{}\".format(seq.shape,w.shape,t.shape))\n",
    "    \n",
    "    # Perform a pass over steps 1-99 or 2-100 in 1-index notation!\n",
    "    for i in range(1,seqLen):\n",
    "        fi  = bMat[i,:]              # get likelihood of i observation being any letter as standalone \n",
    "        cYi = cMat[i-1,:]            # i-1 step's potential msg to this node i\n",
    "        cYj = np.multiply(fi, t)     # mul the likelihood of each letter with all transition probs.\n",
    "        maxIdx[i,:] = np.argmax(cYj, axis = 0) #1x26 the max elem indexes\n",
    "        cYj = cYj[maxIdx[i,:],range(cYj.shape[1])] #1x26 select the max element, as indexed by maxIdx, for every column\n",
    "        cYj = np.multiply(cYj, cYi)  # multiply current cYj = g_yj->yi* fj with pervious node's(i-1) msgs to yield the msg node i will wend to i+1\n",
    "        if i == 1:\n",
    "            print(cYj, cYj.shape, fi.shape)\n",
    "        #mostProbState = np.matmul(res[i-1,:], t)\n",
    "    #TODO: Perform the backward pass to get most prob sequence\n",
    "        \n",
    "    #labels = np.argmax(s, axis = 1)\n",
    "    #result = [letters[i] for i in labels]\n",
    "    return decSeq\n",
    "    \n",
    "crf_decoder(inX, inW, inT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['f', 'p', 'd', 'k', 'j', 'o', 'i', 'u', 'y', 't', 'g', 'j', 'r', 'a', 'v', 'm', 'j', 'm', 's', 'u', 'n', 'h', 'y', 'n', 'p', 'w', 'h', 'm', 'e', 'v', 'j', 'k', 'z', 'p', 'n', 'b', 'i', 'r', 'b', 'z', 'g', 'v', 'm', 'k', 'd', 'd', 'i', 'e', 'x', 'r', 'f', 'e', 'y', 'm', 'm', 'x', 'c', 'o', 'm', 'r', 'x', 'x', 'x', 'n', 'c', 's', 'k', 'v', 'n', 'r', 'u', 'a', 'y', 'o', 'k', 'y', 'r', 'p', 'h', 'l', 'b', 'j', 't', 'w', 'l', 'u', 'w', 'p', 'q', 'a', 'i', 'g', 'r', 'f', 'v', 'u', 'd', 'z', 'a', 'p']\n"
     ]
    }
   ],
   "source": [
    "# Write the decoded results to file\n",
    "# Create results dir if ti does not exists (Python3.2+)\n",
    "resPath = os.path.join(dir_path, 'results')\n",
    "os.makedirs(resPath, exist_ok=True) \n",
    "# Form file name and write output\n",
    "resFile = os.path.join(resPath, 'decode_output.txt')\n",
    "np.savetxt(resFile, decSeq.astype(int),fmt='%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 3 Benchamrking with SVM!\n",
    "modelLabels = ['CRF', 'SVM-HMM', 'SVM-MC']\n",
    "# Define where svm_hmm lib and executables are\n",
    "libPath = os.path.join(dir_path,'svm_hmm_linux64')\n",
    "learnerPath = os.path.join(libPath, 'svm_hmm_learn')\n",
    "testerPath  = os.path.join(libPath, 'svm_hmm_classify')\n",
    "# data for the svms\n",
    "svmTrainData = os.path.join(dataPath,'train_struct.txt')\n",
    "svmTestData = os.path.join(dataPath,'test_struct.txt')\n",
    "# make a file of true classes if they dont exists, without hte other stuff\n",
    "sanTruthFile = os.path.join(dataPath, 'sanitizedTruth.txt')\n",
    "# Get just the true labels and word indexes for all letters without features and the rest\n",
    "if  not os.path.exists(sanThruthFile):\n",
    "    #os.makedir(sanThruthFile)\n",
    "    with open(svmTestData,'r') as o, open(sanTruthFile,'w') as s:\n",
    "        d =[x.split(' ') for x in o.readlines()]   # split file into a list of lists of tokens\n",
    "        sanLabel =[x[0] for x in d]                # get all labels. Its the first token of each line\n",
    "        sanWIdx = [x[1].split(':')[1] for x in d]  # get all word indexes. It is the number after the : of the second token.\n",
    "        # using list comprehension + zip() \n",
    "        # interlist element concatenation \n",
    "        sanData = [' '.join((i,j)) for i, j in zip(sanLabel, sanWIdx)]  \n",
    "        print(sanData[0:30])\n",
    "        s.write(\"\\n\".join(sanData))\n",
    "        \n",
    "gTruth = np.asarray([x.split(' ')[0] for x in open(sanTruthFile).readlines()]) # keep in mind that readlines retunrs the \\n character"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sanTruthFile' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-123-d2eb0d902429>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# array that holds accuracies for all plots. its going to be 2x3x|cParmas| = letter, word-wise accuracy x 3 models x number of examined c Params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcParams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mgTruth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msanTruthFile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# keep in mind that readlines retunrs the \\n character\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcParams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mlearnOptions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'-c '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sanTruthFile' is not defined"
     ]
    }
   ],
   "source": [
    "# C parameter for regulirization impact.\n",
    "cParams = [1,2,5,10,100]\n",
    "# array that holds accuracies for all plots. its going to be 2x3x|cParmas| = letter, word-wise accuracy x 3 models x number of examined c Params\n",
    "# 1 dim: 0=letter-wise acc, 1 = word-wise acc\n",
    "# 2 dim: 0=CRF, 1=SVM HMM, 2 = SVM-MC\n",
    "# 3 dim: the various c Params\n",
    "acc = np.zeros((2,3,len(cParams)))\n",
    "for i,c in enumerate(cParams):\n",
    "    learnOptions = '-c ' +str(c)\n",
    "    svmHmmOutput = 'models/svm_hmm_'+str(c)+'.model'\n",
    "    ## Create a command to call the binaries required\n",
    "    cmd = ' '.join((learnerPath, learnOptions, svmTrainData, svmHmmOutput)) # ./svm_hmm_learn -c 5 -e 0.5 example7/declaration_of_independence.dat declaration.model \n",
    "    outFile = 'results/test_c_'+str(c)+'.outtags' \n",
    "    cmdTest = ' '.join((testerPath, svmTestData, svmHmmOutput, outFile)) \n",
    "    #y, x = lbl.svm_read_problem(svmTrainData, return_scipy = True) # y: ndarray, x: csr_matrix \n",
    "    #m = lbl.train(y[:200], x[:200, :], '-c 4')\n",
    "    print(\"Executing: \" ,cmd)\n",
    "    exit = os.system(cmd) # returns the exit status\n",
    "    os.system(cmdTest) # returns the exit status\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_letterwise_accuracy(pred, gTruth = None):\n",
    "    acc = 0\n",
    "    #if isinstance(type(gTruth), typedir) and isinstance(type(pred),dir):\n",
    "    if gTruth is None:\n",
    "        gTruth = '../data/sanitizedTruth.txt'\n",
    "    elif isinstance(gTruth, np.ndarray):\n",
    "        trueClass = gTruth\n",
    "    else:\n",
    "        with open(gTruth, 'r') as g:\n",
    "            trueClass = np.asarray([x.split(' ')[0] for x in g.readlines()]) # keep in mind that readlines retunrs the \\n character\n",
    "            \n",
    "    with open(pred, 'r') as p:\n",
    "        predClass = np.asarray(p.read().splitlines())\n",
    "        \n",
    "    acc = np.sum(predClass == trueClass) / trueClass.shape[0]\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy results for SVM HMM, for C: [1, 2, 5, 10, 100]:\n",
      " [0.65222536 0.67982289 0.72070387 0.75043896 0.82345981]\n"
     ]
    }
   ],
   "source": [
    "# Compute and print results\n",
    "for i,c in enumerate(cParams):\n",
    "    outFile = 'results/test_c_'+str(c)+'.outtags' \n",
    "    acc[0,1,i] = compute_letterwise_accuracy(outFile, gTruth = gTruth)\n",
    "print(\"Accuracy results for SVM HMM, for C: {}:\\n {}\".format(cParams, acc[0,1,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
